{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Gene, Variation, Text)[INPUT] ---- > [ML model] ----> [OUTPUT](Cancer type label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* No low-latency requirement.\n",
    "* Interpretability is important. (To know reaon of cancer)\n",
    "* Errors can be very costly.\n",
    "* Probability of a data-point belonging to each class is needed.\n",
    "* There are nine different classes a genetic mutation  => Multi class classification problem\n",
    "* Since output are based on probabilities -- > Log Loss and Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings   # to ignore the various warning poping up while block execution\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data from CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID    Gene             Variation  Class\n",
      "0   0  FAM58A  Truncating Mutations      1\n",
      "1   1     CBL                 W802*      2\n",
      "2   2     CBL                 Q249E      2\n",
      "3   3     CBL                 N454D      3\n",
      "4   4     CBL                 L399V      4\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3321 entries, 0 to 3320\n",
      "Data columns (total 4 columns):\n",
      "ID           3321 non-null int64\n",
      "Gene         3321 non-null object\n",
      "Variation    3321 non-null object\n",
      "Class        3321 non-null int64\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 103.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Reading  data from the csv files\n",
    "df_variants = pd.read_csv('training_variants')\n",
    "print(df_variants.head())\n",
    "df_variants.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID                                               TEXT\n",
      "0   0  Cyclin-dependent kinases (CDKs) regulate a var...\n",
      "1   1   Abstract Background  Non-small cell lung canc...\n",
      "2   2   Abstract Background  Non-small cell lung canc...\n",
      "3   3  Recent evidence has demonstrated that acquired...\n",
      "4   4  Oncogenic mutations in the monomeric Casitas B...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3321 entries, 0 to 3320\n",
      "Data columns (total 2 columns):\n",
      "ID      3321 non-null int64\n",
      "TEXT    3316 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 52.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_text =pd.read_csv(\"training_text\",sep=\"\\|\\|\",engine=\"python\",names=[\"ID\",\"TEXT\"],skiprows=1)\n",
    "print(df_text.head())\n",
    "df_text.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Preprocessing and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re # for regular expression\n",
    "import time # to see the execution time of cell, start_time = time.clock(), end_time = time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"Function to clean the text by using regular exp and removing stopwords\"\"\"\n",
    "    if type(text) == type('a'):\n",
    "        text = re.sub(re.compile(r'[^a-zA-Z0-9\\n]'), \" \", text)  # replacing the special character\n",
    "        text = re.sub(re.compile(r'\\s+'), \" \", text)\n",
    "        text = text.lower()\n",
    "        \n",
    "        clean_text = \"\"\n",
    "        for word in text.split():\n",
    "            if word not in stop_words:\n",
    "                clean_text = clean_text + word + \" \"         \n",
    "        return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run the block =  166.7357358  secs\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "\n",
    "df_text.TEXT = df_text.TEXT.apply(text_preprocessing)\n",
    "\n",
    "end_time = time.clock()\n",
    "print(\"Time taken to run the block = \",end_time-start_time,\" secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>cyclin dependent kinases cdks regulate variety...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>abstract background non small cell lung cancer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>abstract background non small cell lung cancer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>recent evidence demonstrated acquired uniparen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>oncogenic mutations monomeric casitas b lineag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                               TEXT\n",
       "0   0  cyclin dependent kinases cdks regulate variety...\n",
       "1   1  abstract background non small cell lung cancer...\n",
       "2   2  abstract background non small cell lung cancer...\n",
       "3   3  recent evidence demonstrated acquired uniparen...\n",
       "4   4  oncogenic mutations monomeric casitas b lineag..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprcoessing for df_variants\n",
    "\n",
    "df_variants.Gene = df_variants.Gene.str.replace('\\s+','_')\n",
    "df_variants.Variation = df_variants.Variation.str.replace('\\s+','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "      <th>Class</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>FAM58A</td>\n",
       "      <td>Truncating_Mutations</td>\n",
       "      <td>1</td>\n",
       "      <td>cyclin dependent kinases cdks regulate variety...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CBL</td>\n",
       "      <td>W802*</td>\n",
       "      <td>2</td>\n",
       "      <td>abstract background non small cell lung cancer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CBL</td>\n",
       "      <td>Q249E</td>\n",
       "      <td>2</td>\n",
       "      <td>abstract background non small cell lung cancer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>CBL</td>\n",
       "      <td>N454D</td>\n",
       "      <td>3</td>\n",
       "      <td>recent evidence demonstrated acquired uniparen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>CBL</td>\n",
       "      <td>L399V</td>\n",
       "      <td>4</td>\n",
       "      <td>oncogenic mutations monomeric casitas b lineag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID    Gene             Variation  Class  \\\n",
       "0   0  FAM58A  Truncating_Mutations      1   \n",
       "1   1     CBL                 W802*      2   \n",
       "2   2     CBL                 Q249E      2   \n",
       "3   3     CBL                 N454D      3   \n",
       "4   4     CBL                 L399V      4   \n",
       "\n",
       "                                                TEXT  \n",
       "0  cyclin dependent kinases cdks regulate variety...  \n",
       "1  abstract background non small cell lung cancer...  \n",
       "2  abstract background non small cell lung cancer...  \n",
       "3  recent evidence demonstrated acquired uniparen...  \n",
       "4  oncogenic mutations monomeric casitas b lineag...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging both  dataframes\n",
    "\n",
    "df = pd.merge(df_variants,df_text,on='ID',how='left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "      <th>Class</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>1109</td>\n",
       "      <td>FANCA</td>\n",
       "      <td>S1088F</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>1277</td>\n",
       "      <td>ARID5B</td>\n",
       "      <td>Truncating_Mutations</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>1407</td>\n",
       "      <td>FGFR3</td>\n",
       "      <td>K508M</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1639</th>\n",
       "      <td>1639</td>\n",
       "      <td>FLT1</td>\n",
       "      <td>Amplification</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2755</th>\n",
       "      <td>2755</td>\n",
       "      <td>BRAF</td>\n",
       "      <td>G596C</td>\n",
       "      <td>7</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID    Gene             Variation  Class  TEXT\n",
       "1109  1109   FANCA                S1088F      1  None\n",
       "1277  1277  ARID5B  Truncating_Mutations      1  None\n",
       "1407  1407   FGFR3                 K508M      6  None\n",
       "1639  1639    FLT1         Amplification      6  None\n",
       "2755  2755    BRAF                 G596C      7  None"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking and filling null values of text\n",
    "\n",
    "df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling the null values with the 'gene + Variation text'\n",
    "df.loc[df['TEXT'].isnull(),'TEXT'] = df['Gene'] +' '+df['Variation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3321 entries, 0 to 3320\n",
      "Data columns (total 5 columns):\n",
      "ID           3321 non-null int64\n",
      "Gene         3321 non-null object\n",
      "Variation    3321 non-null object\n",
      "Class        3321 non-null int64\n",
      "TEXT         3321 non-null object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 315.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Dataset into Train, CV and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# giving \"stratify = df['Class']\" will maintain the random distribution of dataset a/c to df['Class']  \n",
    "\n",
    "x_df , x_test, y_df ,y_test = train_test_split(df, df['Class'], stratify = df['Class'].values, test_size=0.20)\n",
    "\n",
    "# again breaking for train and CV\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(x_df, y_df, stratify = y_df, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    609\n",
       "4    439\n",
       "1    363\n",
       "2    289\n",
       "6    176\n",
       "5    155\n",
       "3     57\n",
       "9     24\n",
       "8     12\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    191\n",
       "4    137\n",
       "1    114\n",
       "2     91\n",
       "6     55\n",
       "5     48\n",
       "3     18\n",
       "9      7\n",
       "8      4\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    153\n",
       "4    110\n",
       "1     91\n",
       "2     72\n",
       "6     44\n",
       "5     39\n",
       "3     14\n",
       "9      6\n",
       "8      3\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cv.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above distributions also shows that the dataset in Imbalanceed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for plotting Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_test, y_predicted):\n",
    "   \n",
    "    c_labels = [1,2,3,4,5,6,7,8,9]\n",
    "    \n",
    "        \n",
    "    # Confusion Matrix and its presentation\n",
    "    confusion = confusion_matrix(y_test, y_predicted)\n",
    "    \n",
    "    print(\"-\"*20, \"Confusion matrix\", \"-\"*20)\n",
    "    plt.figure(figsize=(20,7))\n",
    "    sns.heatmap(confusion, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.show()\n",
    "    \n",
    "    # Precision Matrix\n",
    "    precision = confusion/confusion.sum(axis=0)\n",
    "\n",
    "    print(\"-\"*20, \"Presion matrix\", \"-\"*20)\n",
    "    plt.figure(figsize=(20,7))\n",
    "    sns.heatmap(precision, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels = c_labels, yticklabels= c_labels)\n",
    "    plt.xlabel(\"Predcited Class\")\n",
    "    plt.ylabel(\"Original Class\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Recall Matrix\n",
    "    recall = ((confusion.T)/((confusion.T).sum(axis=0))).T\n",
    "    \n",
    "    print(\"-\"*20, \"Recall matrix\", \"-\"*20)\n",
    "    plt.figure(figsize=(20,7))\n",
    "    sns.heatmap(recall, annot=True, cmap=\"YlGnBu\", fmt=\".3f\",xticklabels = c_labels, yticklabels= c_labels )\n",
    "    plt.xlabel(\"Predcited Class\")\n",
    "    plt.ylabel(\"Original Class\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting using Random Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Feature -- > Numerical Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of unique genes -  264\n",
      "\n",
      "Top 10 genes - \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BRCA1     264\n",
       "TP53      163\n",
       "EGFR      141\n",
       "PTEN      126\n",
       "BRCA2     125\n",
       "KIT        99\n",
       "BRAF       93\n",
       "ERBB2      69\n",
       "ALK        69\n",
       "PDGFRA     60\n",
       "Name: Gene, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"No of unique genes - \",df.Gene.value_counts().shape[0])\n",
    "print(\"\\nTop 10 genes - \")\n",
    "df.Gene.value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 264 type of genes\n",
    "# Methods available for encoding --\n",
    "# 1. one hot encoding  - for (Logistic regression , linear svm)\n",
    "# 2. Response coding  - for (random foredt, Naive Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  when we caculate the probability of a feature belongs to any particular class,also by apply laplace smoothing\n",
    "#  prob = (numerator + 10\\*alpha) / (denominator + 90\\*alpha)\n",
    "\n",
    "def prob_count(dataframe, feature, alpha):\n",
    "    feature_dict = dict()\n",
    "    for gene_type, no_of_gene in dataframe[feature].value_counts().items():\n",
    "        prob_vec = []\n",
    "        for i in range(1,10):\n",
    "            no_class_type = dataframe[(dataframe[feature]==gene_type) & (dataframe.Class==i)].shape[0]\n",
    "            prob = (int(no_class_type) + alpha*10)/(int(no_of_gene) + 90*alpha)\n",
    "            prob_vec.append(prob)\n",
    "        feature_dict[gene_type]= prob_vec\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_apply(dataframe, feature, alpha):\n",
    "    \n",
    "    dic = prob_count(dataframe,feature, alpha)\n",
    "    \n",
    "    value_count = dataframe[feature].value_counts()\n",
    "    gv_fea = []\n",
    "   \n",
    "    for index, row in dataframe.iterrows():\n",
    "        if row[feature] in dict(value_count).keys():\n",
    "            gv_fea.append(dic[row[feature]])\n",
    "        else:\n",
    "            gv_fea.append([1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9])\n",
    "    return gv_fea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference between the Onehot and Resemble encoding dimensionality \n",
    "\n",
    "* Response Coding shape -- (2124 , 9 )\n",
    "* One hot encoding shape -- (2124 , 230 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariant Analysis on feature -- \"Gene\"\n",
    "\n",
    "* To check the imporatance of \"Gene\" feature in predicting the model\n",
    "* Train a simple model by taking \"Gene\" as only feature\n",
    "* Compare the Logg loss of above model with Random Model\n",
    "* If log loss Simple model < log loss Random model, then \"Gene\" can be considered useful feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverting the features of dataset onto Response Coding --\n",
    "\n",
    "train_response_coded = np.array(feature_apply(x_train, \"Gene\", 1))\n",
    "test_response_coded = np.array(feature_apply(x_test, \"Gene\", 1))\n",
    "cv_response_coded = np.array(feature_apply(x_cv, \"Gene\", 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2124, 9), (665, 9), (532, 9))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_response_coded.shape , test_response_coded.shape , cv_response_coded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding of the \"Gene\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "gene_vectorizer = CountVectorizer()\n",
    "train_gene_onehot = gene_vectorizer.fit_transform(x_train.Gene)\n",
    "test_gene_onehot = gene_vectorizer.transform(x_test.Gene)\n",
    "cv_gene_onehot = gene_vectorizer.transform(x_cv.Gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2124, 230), (665, 230), (532, 230))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gene_onehot.shape,  test_gene_onehot.shape, cv_gene_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV # probalistic predicted model needed to be calibrated\n",
    "from sklearn.metrics.classification import accuracy_score, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss for aplha =  1e-05  is  1.3917533656368415\n",
      "Log loss for aplha =  0.0001  is  1.4083940869639775\n",
      "Log loss for aplha =  0.001  is  1.4703591539071679\n",
      "Log loss for aplha =  0.01  is  1.502269165547802\n",
      "Log loss for aplha =  0.1  is  1.5080843250146783\n",
      "Log loss for aplha =  1  is  1.5047469553775108\n",
      "Log loss for aplha =  10  is  1.477187220895148\n"
     ]
    }
   ],
   "source": [
    "# implementing LOGISTIC REGRESSION in SGDClassifier --\n",
    "# First with Response Encoding\n",
    "\n",
    "alpha = [10**x for x in range(-5,2)]\n",
    "\n",
    "log_loss_array = []\n",
    "\n",
    "for i in alpha:\n",
    "    model = SGDClassifier(loss='log',penalty='l2',alpha=i, random_state=42)\n",
    "    model.fit(train_response_coded ,y_train)\n",
    "    sigmoid_model = CalibratedClassifierCV( model , method='sigmoid')\n",
    "    sigmoid_model.fit(train_response_coded,y_train)\n",
    "    predict_y = sigmoid_model.predict_proba(cv_response_coded)\n",
    "    log_loss_array.append(log_loss(y_cv, predict_y))\n",
    "    print(\"Log loss for aplha = \",i,\" is \",log_loss(y_cv, predict_y, labels=model.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For values of best alpha =  1e-05  The train log loss is: 1.171074312545567\n",
      "For values of best alpha =  1e-05  The cross validation log loss is: 1.3917533656368415\n",
      "For values of best alpha =  1e-05  The test log loss is: 1.3713953775684529\n"
     ]
    }
   ],
   "source": [
    "# trainning model with best alpha = 10**-5\n",
    "alpha=1e-05\n",
    "model = SGDClassifier(loss='log',penalty='l2',alpha=1e-05,random_state=42)\n",
    "model.fit(train_response_coded, y_train)\n",
    "sigmoid_model = CalibratedClassifierCV(model,method='sigmoid')\n",
    "sigmoid_model.fit(train_response_coded, y_train)\n",
    "\n",
    "\n",
    "predict_y  = sigmoid_model.predict_proba(train_response_coded)\n",
    "print('For values of best alpha = ', alpha, \" The train log loss is:\",log_loss(y_train, predict_y, labels=model.classes_, eps=1e-15))\n",
    "predict_y  = sigmoid_model.predict_proba(cv_response_coded)\n",
    "print('For values of best alpha = ', alpha, \" The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=model.classes_, eps=1e-15))\n",
    "predict_y  = sigmoid_model.predict_proba(test_response_coded)\n",
    "print('For values of best alpha = ', alpha, \" The test log loss is:\",log_loss(y_test, predict_y, labels=model.classes_, eps=1e-15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is not much difference between the train an test log loss and much less then Random model logg loss, this shows ---\n",
    "#  1) \"Gene\" feature is useful\n",
    "#  2) Model is not overfit,\n",
    "# Now Trying to reduce the loss furthus reducing the overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss for aplha =  1e-05  is  1.3971465399382268\n",
      "Log loss for aplha =  0.0001  is  1.235717148338864\n",
      "Log loss for aplha =  0.001  is  1.2700875730695023\n",
      "Log loss for aplha =  0.01  is  1.3780895104375896\n",
      "Log loss for aplha =  0.1  is  1.4545261923537602\n",
      "Log loss for aplha =  1  is  1.4792869379682054\n",
      "Log loss for aplha =  10  is  1.4828140475194644\n"
     ]
    }
   ],
   "source": [
    "# Implementing Linear regg with Onehot Encoding\n",
    "alpha = [10**x for x in range(-5,2)]\n",
    "log_loss_array = []\n",
    "\n",
    "for i in alpha:\n",
    "    model = SGDClassifier(loss=\"log\",penalty='l2',alpha=i,random_state=42)\n",
    "    model.fit(train_gene_onehot, y_train)\n",
    "    sigmoid_model = CalibratedClassifierCV(model, method='sigmoid')\n",
    "    sigmoid_model.fit(train_gene_onehot, y_train)\n",
    "    predict_y = sigmoid_model.predict_proba(cv_gene_onehot)\n",
    "    log_loss_array.append(log_loss(y_cv, predict_y, labels=model.classes_))\n",
    "    print(\"Log loss for aplha = \",i,\" is \",log_loss(y_cv, predict_y, labels=model.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariant Analysis on feature -- \"Variation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique variations =  1935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Truncating_Mutations    63\n",
       "Deletion                45\n",
       "Amplification           44\n",
       "Fusions                 23\n",
       "Overexpression           3\n",
       "Name: Variation, dtype: int64"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of unique variations = \",x_train.Variation.value_counts().shape[0])\n",
    "x_train.Variation.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting HISTOGRAM to see the distribution of variations\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "s = sum(x_train.Variation.value_counts().values);\n",
    "h = x_train.Variation.value_counts().values/s;\n",
    "plt.plot(h, label=\"Histrogram of Variations\")\n",
    "plt.xlabel('Index of a Variation')\n",
    "plt.ylabel('Number of Occurances')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = np.cumsum(h)\n",
    "plt.plot(c,label='Cumulative distribution of Variations')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "'''curve is almost staright linear line, this sows if we select lets top 1500 feature\n",
    "   a/c to the y-axis prob reading = 80%, i.e. top 1500 features out of 1925 only contributes 80%\n",
    "   this shows most variation occur once'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods available to featurize :\n",
    "# 1) One hot Encoding\n",
    "# 2) Response coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response Coding\n",
    "alpha = 1 # taken for laplace smoothing\n",
    "\n",
    "train_var_res  = np.array(feature_apply(x_train,'Variation',1 ))\n",
    "test_var_res = np.array(feature_apply(x_test,'Variation',1))\n",
    "cv_var_res = np.array(feature_apply(x_cv,'Variation',1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2124, 9), (665, 9), (532, 9))"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_var_res.shape, test_var_res.shape, cv_var_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding of variation feature.\n",
    "variation_vectorizer = CountVectorizer()\n",
    "train_variation_feature_onehotCoding = variation_vectorizer.fit_transform(x_train['Variation'])\n",
    "test_variation_feature_onehotCoding = variation_vectorizer.transform(x_test['Variation'])\n",
    "cv_variation_feature_onehotCoding = variation_vectorizer.transform(x_cv['Variation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2124, 1962), (665, 1962), (532, 1962))"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_variation_feature_onehotCoding.shape , test_variation_feature_onehotCoding.shape,  cv_variation_feature_onehotCoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1962"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(variation_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss for aplha =  1e-05  is  1.7349383791868123\n",
      "Log loss for aplha =  0.0001  is  1.7221588589493528\n",
      "Log loss for aplha =  0.001  is  1.7216159120667356\n",
      "Log loss for aplha =  0.01  is  1.727535368297859\n",
      "Log loss for aplha =  0.1  is  1.7350697615182857\n",
      "Log loss for aplha =  1  is  1.7364241940001783\n",
      "Log loss for aplha =  10  is  1.736589177903987\n"
     ]
    }
   ],
   "source": [
    "# trainning model on logistic regg using one hot encoded data\n",
    "\n",
    "alpha = [10**x for x in range(-5,2)]\n",
    "log_loss_array = []\n",
    "\n",
    "for i in alpha:\n",
    "    model = SGDClassifier(loss='log', penalty='l2',alpha=i, random_state=42)\n",
    "    model.fit(train_variation_feature_onehotCoding, y_train)\n",
    "    sigmoid_model = CalibratedClassifierCV(model, method='sigmoid')\n",
    "    sigmoid_model.fit(train_variation_feature_onehotCoding, y_train)\n",
    "    predict_y = sigmoid_model.predict_proba(cv_variation_feature_onehotCoding)\n",
    "    log_loss_array.append(log_loss(y_cv, predict_y))\n",
    "    print(\"Log loss for aplha = \",i,\" is \",log_loss(y_cv, predict_y, labels=model.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For values of best alpha =  0.001  The train log loss is: 1.1163090624636873\n",
      "For values of best alpha =  0.001  The cross validation log loss is: 1.7216159120667356\n",
      "For values of best alpha =  0.001  The test log loss is: 1.7150314078187232\n"
     ]
    }
   ],
   "source": [
    "# trainning model at best alpha = 0.001\n",
    "alpha = 0.001\n",
    "\n",
    "model = SGDClassifier(loss='log',penalty='l2',alpha=0.001,random_state=42)\n",
    "model.fit(train_variation_feature_onehotCoding, y_train)\n",
    "sigmoid_model = CalibratedClassifierCV(model,method='sigmoid')\n",
    "sigmoid_model.fit(train_variation_feature_onehotCoding, y_train)\n",
    "\n",
    "\n",
    "predict_y  = sigmoid_model.predict_proba(train_variation_feature_onehotCoding)\n",
    "\n",
    "print('For values of best alpha = ', alpha, \" The train log loss is:\",log_loss(y_train, predict_y, labels=model.classes_, eps=1e-15))\n",
    "predict_y  = sigmoid_model.predict_proba(cv_variation_feature_onehotCoding)\n",
    "print('For values of best alpha = ', alpha, \" The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=model.classes_, eps=1e-15))\n",
    "predict_y  = sigmoid_model.predict_proba(test_variation_feature_onehotCoding)\n",
    "print('For values of best alpha = ', alpha, \" The test log loss is:\",log_loss(y_test, predict_y, labels=model.classes_, eps=1e-15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Conlusions from 'Variation' analysis-\n",
    "# 1) Variation feature is useful as loss less then random model\n",
    "# 2) More instability in variation then gene feature as in variation train, cv and test loss have marginal difference\n",
    "# 3) Gene more stable as compared to variation\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariant Analysis on feature -- \"Text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2881    abstract classification rare missense variants...\n",
       "1295    three dimensional structure complex human h ra...\n",
       "1491    abstract prospective clinical sequencing progr...\n",
       "2905    neurofibromatosis type 2 nf2 multiple neoplasi...\n",
       "343     41 year old man familial history gastric cance...\n",
       "Name: TEXT, dtype: object"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.TEXT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functiion to count the no of words in each row\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_dictionary_paddle(cls_text):\n",
    "    dictionary = defaultdict(int)\n",
    "    for index, row in cls_text.iterrows():\n",
    "        for word in row['TEXT'].split():\n",
    "            dictionary[word] +=1\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_text_responsecoding(df):\n",
    "    text_feature_responseCoding = np.zeros((df.shape[0],9))\n",
    "    for i in range(0,9):\n",
    "        row_index = 0\n",
    "        for index, row in df.iterrows():\n",
    "            sum_prob = 0\n",
    "            for word in row['TEXT'].split():\n",
    "                sum_prob += math.log(((dict_list[i].get(word,0)+10 )/(total_dict.get(word,0)+90)))\n",
    "            text_feature_responseCoding[row_index][i] = math.exp(sum_prob/len(row['TEXT'].split()))\n",
    "            row_index += 1\n",
    "    return text_feature_responseCoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words in train data : 53994\n"
     ]
    }
   ],
   "source": [
    "# Text feature one hot incoding- \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text_vectorizer = CountVectorizer(min_df=3)\n",
    "train_text_feature_onehotCoding = text_vectorizer.fit_transform(x_train.TEXT)\n",
    "\n",
    "train_text_features = text_vectorizer.get_feature_names()\n",
    "# train_text_feature_onehotCoding.sum(axis=0).A1 will sum every row and returns (1*number of features) vector\n",
    "train_text_fea_counts = train_text_feature_onehotCoding.sum(axis=0).A1\n",
    "\n",
    "# zip(list(text_features),text_fea_counts) will zip a word with its number of times it occured\n",
    "text_fea_dict = dict(zip(list(train_text_features),train_text_fea_counts))\n",
    "\n",
    "\n",
    "print(\"Total number of unique words in train data :\", len(train_text_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2124, 53994)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_onehot_encode.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list = []\n",
    "# dict_list =[] contains 9 dictoinaries each corresponds to a class\n",
    "for i in range(1,10):\n",
    "    cls_text = x_train[x_train['Class']==i]\n",
    "    # build a word dict based on the words in that class\n",
    "    dict_list.append(extract_dictionary_paddle(cls_text))\n",
    "    # append it to dict_list\n",
    "\n",
    "# dict_list[i] is build on i'th  class text data\n",
    "# total_dict is buid on whole training text data\n",
    "total_dict = extract_dictionary_paddle(x_train)\n",
    "\n",
    "\n",
    "confuse_array = []\n",
    "for i in train_text_features:\n",
    "    ratios = []\n",
    "    max_val = -1\n",
    "    for j in range(0,9):\n",
    "        ratios.append((dict_list[j][i]+10 )/(total_dict[i]+90))\n",
    "    confuse_array.append(ratios)\n",
    "confuse_array = np.array(confuse_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response coding of text features\n",
    "train_text_feature_responseCoding  = get_text_responsecoding(x_train)\n",
    "test_text_feature_responseCoding  = get_text_responsecoding(x_test)\n",
    "cv_text_feature_responseCoding  = get_text_responsecoding(x_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we convert each row values such that they sum to 1  \n",
    "train_text_feature_responseCoding = (train_text_feature_responseCoding.T/train_text_feature_responseCoding.sum(axis=1)).T\n",
    "test_text_feature_responseCoding = (test_text_feature_responseCoding.T/test_text_feature_responseCoding.sum(axis=1)).T\n",
    "cv_text_feature_responseCoding = (cv_text_feature_responseCoding.T/cv_text_feature_responseCoding.sum(axis=1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "# Normalizing every vector\n",
    "\n",
    "train_text_feature_onehotCoding = normalize(train_text_feature_onehotCoding, axis=0)\n",
    "\n",
    "# we use the same vectorizer that was trained on train data\n",
    "test_text_feature_onehotCoding = text_vectorizer.transform(x_test['TEXT'])\n",
    "# don't forget to normalize every feature\n",
    "test_text_feature_onehotCoding = normalize(test_text_feature_onehotCoding, axis=0)\n",
    "\n",
    "# we use the same vectorizer that was trained on train data\n",
    "cv_text_feature_onehotCoding = text_vectorizer.transform(x_cv['TEXT'])\n",
    "# don't forget to normalize every feature\n",
    "cv_text_feature_onehotCoding = normalize(cv_text_feature_onehotCoding, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_text_fea_dict = dict(sorted(text_fea_dict.items(), key=lambda x: x[1] , reverse=True))\n",
    "sorted_text_occur = np.array(list(sorted_text_fea_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For values of alpha =  1e-05 The log loss is: 1.3650447261379834\n",
      "For values of alpha =  0.0001 The log loss is: 1.3170905152904284\n",
      "For values of alpha =  0.001 The log loss is: 1.1756495758154049\n",
      "For values of alpha =  0.01 The log loss is: 1.258298277466114\n",
      "For values of alpha =  0.1 The log loss is: 1.4648553813858824\n",
      "For values of alpha =  1 The log loss is: 1.6712464013672659\n",
      "For values of alpha =  10 The log loss is: 1.7044766449267803\n"
     ]
    }
   ],
   "source": [
    "alpha = [10**x for x in range(-5,2)]\n",
    "cv_log_error_array=[]\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n",
    "    clf.fit(train_text_feature_onehotCoding, y_train)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(train_text_feature_onehotCoding, y_train)\n",
    "    predict_y = sig_clf.predict_proba(cv_text_feature_onehotCoding)\n",
    "    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For values of best alpha =  0.001 The train log loss is: 0.7849638132676389\n",
      "For values of best alpha =  0.001 The cross validation log loss is: 1.1756495758154049\n",
      "For values of best alpha =  0.001 The test log loss is: 1.1425619381385108\n"
     ]
    }
   ],
   "source": [
    "# The best alpha = 0.001\n",
    "alpha = 0.001\n",
    "\n",
    "# trainning model on best alpha\n",
    "\n",
    "best_alpha = np.argmin(cv_log_error_array)\n",
    "clf = SGDClassifier(alpha=0.001, penalty='l2', loss='log', random_state=42)\n",
    "clf.fit(train_text_feature_onehotCoding, y_train)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(train_text_feature_onehotCoding, y_train)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(train_text_feature_onehotCoding)\n",
    "print('For values of best alpha = ', alpha, \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(cv_text_feature_onehotCoding)\n",
    "print('For values of best alpha = ', alpha, \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(test_text_feature_onehotCoding)\n",
    "print('For values of best alpha = ', alpha, \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following to be asked from all features - \n",
    "1. How many unique words are present in train data?\n",
    "2. How are word frequencies distributed?\n",
    "3. How to featurize text field?\n",
    "4. Is the text feature useful in predicitng y_i?\n",
    "5. Is the text feature stable across train, test and CV datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack, vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One hot encoding features :\n",
      "(number of data points * number of features) in train data =  (2124, 56186)\n",
      "(number of data points * number of features) in test data =  (665, 56186)\n",
      "(number of data points * number of features) in cross validation data = (532, 56186)\n"
     ]
    }
   ],
   "source": [
    "# One hot Encoding Data\n",
    "\n",
    "train_gene_var_onehot = hstack((train_gene_onehot,train_variation_feature_onehotCoding))\n",
    "test_gene_var_onehot = hstack((test_gene_onehot,test_variation_feature_onehotCoding))\n",
    "cv_gene_var_onehot = hstack((cv_gene_onehot,cv_variation_feature_onehotCoding))\n",
    "\n",
    "train_x_onehotCoding = hstack((train_gene_var_onehot, train_text_feature_onehotCoding)).tocsr()\n",
    "train_y = np.array(list(x_train['Class']))\n",
    "\n",
    "test_x_onehotCoding = hstack((test_gene_var_onehot, test_text_feature_onehotCoding)).tocsr()\n",
    "test_y = np.array(list(x_test['Class']))\n",
    "\n",
    "cv_x_onehotCoding = hstack((cv_gene_var_onehot, cv_text_feature_onehotCoding)).tocsr()\n",
    "cv_y = np.array(list(x_cv['Class']))\n",
    "\n",
    "print(\"One hot encoding features :\")\n",
    "print(\"(number of data points * number of features) in train data = \", train_x_onehotCoding.shape)\n",
    "print(\"(number of data points * number of features) in test data = \", test_x_onehotCoding.shape)\n",
    "print(\"(number of data points * number of features) in cross validation data =\", cv_x_onehotCoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response Coding Data\n",
    "\n",
    "train_gene_var_responseCoding = np.hstack((train_gene_response,train_var_res))\n",
    "test_gene_var_responseCoding = np.hstack((test_gene_response,test_var_res))\n",
    "cv_gene_var_responseCoding = np.hstack((cv_gene_response,cv_var_res))\n",
    "\n",
    "train_x_responseCoding = np.hstack((train_gene_var_responseCoding, train_text_feature_responseCoding))\n",
    "test_x_responseCoding = np.hstack((test_gene_var_responseCoding, test_text_feature_responseCoding))\n",
    "cv_x_responseCoding = np.hstack((cv_gene_var_responseCoding, cv_text_feature_responseCoding))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bays (Baseline Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB # haning multiple classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss : 1.29794252840723  For alpha =  1e-05\n",
      "Log Loss : 1.295867489977237  For alpha =  0.0001\n",
      "Log Loss : 1.2999103131535357  For alpha =  0.001\n",
      "Log Loss : 1.301798781033563  For alpha =  0.01\n",
      "Log Loss : 1.3196815855225286  For alpha =  0.1\n",
      "Log Loss : 1.3456016304356082  For alpha =  1\n",
      "Log Loss : 1.3964065509815382  For alpha =  10\n"
     ]
    }
   ],
   "source": [
    "alpha = [10**x for x in range(-5,2)]\n",
    "cv_logloss_array = []\n",
    "for i in alpha:\n",
    "    clf = MultinomialNB(alpha=i)\n",
    "    clf.fit(train_x_onehotCoding, train_y)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(train_x_onehotCoding, train_y)\n",
    "    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n",
    "    cv_log_error_array.append(log_loss(cv_y,sig_clf_probs, labels=clf.classes_))\n",
    "    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs),\" For alpha = \",i) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For values of best alpha =  0.0001 The train log loss is: 0.8806747652737554\n",
      "For values of best alpha =  0.0001 The cross validation log loss is: 1.295867489977237\n",
      "For values of best alpha =  0.0001 The test log loss is: 1.2724920025131252\n"
     ]
    }
   ],
   "source": [
    "# best alpha = 0.0001\n",
    "best_alpha = 0.0001\n",
    "clf = MultinomialNB(alpha=best_alpha)\n",
    "clf.fit(train_x_onehotCoding, train_y)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(train_x_onehotCoding, train_y)\n",
    "\n",
    "\n",
    "predict_y = sig_clf.predict_proba(train_x_onehotCoding)\n",
    "print('For values of best alpha = ', best_alpha, \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(cv_x_onehotCoding)\n",
    "print('For values of best alpha = ', best_alpha, \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(test_x_onehotCoding)\n",
    "print('For values of best alpha = ', best_alpha, \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will be used just for naive bayes\n",
    "# for the given indices, we will print the name of the features\n",
    "# and we will check whether the feature present in the test point text or not\n",
    "def get_impfeature_names(indices, text, gene, var, no_features):\n",
    "    gene_count_vec = CountVectorizer()\n",
    "    var_count_vec = CountVectorizer()\n",
    "    text_count_vec = CountVectorizer(min_df=3)\n",
    "    \n",
    "    gene_vec = gene_count_vec.fit(x_train['Gene'])\n",
    "    var_vec  = var_count_vec.fit(x_train['Variation'])\n",
    "    text_vec = text_count_vec.fit(x_train['TEXT'])\n",
    "    \n",
    "    fea1_len = len(gene_vec.get_feature_names())\n",
    "    fea2_len = len(var_count_vec.get_feature_names())\n",
    "    \n",
    "    word_present = 0\n",
    "    for i,v in enumerate(indices):\n",
    "        if (v < fea1_len):\n",
    "            word = gene_vec.get_feature_names()[v]\n",
    "            yes_no = True if word == gene else False\n",
    "            if yes_no:\n",
    "                word_present += 1\n",
    "                print(i, \"Gene feature [{}] present in test data point [{}]\".format(word,yes_no))\n",
    "        elif (v < fea1_len+fea2_len):\n",
    "            word = var_vec.get_feature_names()[v-(fea1_len)]\n",
    "            yes_no = True if word == var else False\n",
    "            if yes_no:\n",
    "                word_present += 1\n",
    "                print(i, \"variation feature [{}] present in test data point [{}]\".format(word,yes_no))\n",
    "        else:\n",
    "            word = text_vec.get_feature_names()[v-(fea1_len+fea2_len)]\n",
    "            yes_no = True if word in text.split() else False\n",
    "            if yes_no:\n",
    "                word_present += 1\n",
    "                print(i, \"Text feature [{}] present in test data point [{}]\".format(word,yes_no))\n",
    "\n",
    "    print(\"Out of the top \",no_features,\" features \", word_present, \"are present in query point\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class : 2\n",
      "Predicted Class Probabilities: [[0.1103 0.5172 0.0131 0.1286 0.0494 0.0421 0.1313 0.0055 0.0025]]\n",
      "Actual Class : 2\n",
      "--------------------------------------------------\n",
      "17 Text feature [novel] present in test data point [True]\n",
      "18 Text feature [identified] present in test data point [True]\n",
      "19 Text feature [molecular] present in test data point [True]\n",
      "23 Text feature [including] present in test data point [True]\n",
      "24 Text feature [time] present in test data point [True]\n",
      "25 Text feature [new] present in test data point [True]\n",
      "26 Text feature [confirmed] present in test data point [True]\n",
      "27 Text feature [another] present in test data point [True]\n",
      "28 Text feature [different] present in test data point [True]\n",
      "29 Text feature [found] present in test data point [True]\n",
      "30 Text feature [recently] present in test data point [True]\n",
      "31 Text feature [well] present in test data point [True]\n",
      "32 Text feature [identification] present in test data point [True]\n",
      "34 Text feature [potential] present in test data point [True]\n",
      "36 Text feature [12] present in test data point [True]\n",
      "37 Text feature [case] present in test data point [True]\n",
      "38 Text feature [sequencing] present in test data point [True]\n",
      "39 Text feature [present] present in test data point [True]\n",
      "40 Text feature [also] present in test data point [True]\n",
      "41 Text feature [kinase] present in test data point [True]\n",
      "42 Text feature [using] present in test data point [True]\n",
      "43 Text feature [gene] present in test data point [True]\n",
      "44 Text feature [detection] present in test data point [True]\n",
      "45 Text feature [15] present in test data point [True]\n",
      "47 Text feature [tissue] present in test data point [True]\n",
      "48 Text feature [highly] present in test data point [True]\n",
      "49 Text feature [number] present in test data point [True]\n",
      "50 Text feature [common] present in test data point [True]\n",
      "51 Text feature [respectively] present in test data point [True]\n",
      "52 Text feature [one] present in test data point [True]\n",
      "53 Text feature [characterized] present in test data point [True]\n",
      "54 Text feature [pcr] present in test data point [True]\n",
      "55 Text feature [subsequent] present in test data point [True]\n",
      "56 Text feature [revealed] present in test data point [True]\n",
      "57 Text feature [findings] present in test data point [True]\n",
      "58 Text feature [therapeutic] present in test data point [True]\n",
      "59 Text feature [may] present in test data point [True]\n",
      "60 Text feature [identify] present in test data point [True]\n",
      "61 Text feature [achieved] present in test data point [True]\n",
      "62 Text feature [rearrangements] present in test data point [True]\n",
      "63 Text feature [initial] present in test data point [True]\n",
      "64 Text feature [additional] present in test data point [True]\n",
      "65 Text feature [described] present in test data point [True]\n",
      "67 Text feature [recent] present in test data point [True]\n",
      "68 Text feature [however] present in test data point [True]\n",
      "69 Text feature [clinical] present in test data point [True]\n",
      "70 Text feature [three] present in test data point [True]\n",
      "71 Text feature [development] present in test data point [True]\n",
      "72 Text feature [small] present in test data point [True]\n",
      "73 Text feature [mutations] present in test data point [True]\n",
      "74 Text feature [although] present in test data point [True]\n",
      "75 Text feature [specific] present in test data point [True]\n",
      "76 Text feature [observed] present in test data point [True]\n",
      "77 Text feature [known] present in test data point [True]\n",
      "78 Text feature [patient] present in test data point [True]\n",
      "79 Text feature [either] present in test data point [True]\n",
      "80 Text feature [studies] present in test data point [True]\n",
      "81 Text feature [similar] present in test data point [True]\n",
      "82 Text feature [previously] present in test data point [True]\n",
      "83 Text feature [13] present in test data point [True]\n",
      "84 Text feature [two] present in test data point [True]\n",
      "87 Text feature [go] present in test data point [True]\n",
      "88 Text feature [discussion] present in test data point [True]\n",
      "89 Text feature [similarly] present in test data point [True]\n",
      "90 Text feature [various] present in test data point [True]\n",
      "91 Text feature [10] present in test data point [True]\n",
      "92 Text feature [distinct] present in test data point [True]\n",
      "93 Text feature [study] present in test data point [True]\n",
      "94 Text feature [performed] present in test data point [True]\n",
      "95 Text feature [cell] present in test data point [True]\n",
      "96 Text feature [cases] present in test data point [True]\n",
      "97 Text feature [samples] present in test data point [True]\n",
      "99 Text feature [reported] present in test data point [True]\n",
      "Out of the top  100  features  73 are present in query point\n"
     ]
    }
   ],
   "source": [
    "test_point_index = 1\n",
    "no_feature = 100\n",
    "predicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\n",
    "print(\"Predicted Class :\", predicted_cls[0])\n",
    "print(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\n",
    "print(\"Actual Class :\", test_y[test_point_index])\n",
    "indices=np.argsort(abs(-clf.coef_))[predicted_cls-1][:,:no_feature]\n",
    "print(\"-\"*50)\n",
    "get_impfeature_names(indices[0], x_train['TEXT'].iloc[test_point_index],x_train['Gene'].iloc[test_point_index],x_train['Variation'].iloc[test_point_index], no_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for alpha = 5\n",
      "Log Loss : 0.9767519132080404\n",
      "for alpha = 11\n",
      "Log Loss : 1.0141874695062616\n",
      "for alpha = 15\n",
      "Log Loss : 1.02672583597814\n",
      "for alpha = 21\n",
      "Log Loss : 1.0405414165225093\n",
      "for alpha = 31\n",
      "Log Loss : 1.065371124892595\n",
      "for alpha = 41\n",
      "Log Loss : 1.074852103802159\n",
      "for alpha = 51\n",
      "Log Loss : 1.086165338442726\n",
      "for alpha = 99\n",
      "Log Loss : 1.1616878969128774\n",
      "For values of best alpha =  5 The train log loss is: 0.4705959631023044\n",
      "For values of best alpha =  5 The cross validation log loss is: 0.9767519132080404\n",
      "For values of best alpha =  5 The test log loss is: 0.9266963206170508\n"
     ]
    }
   ],
   "source": [
    "# KNN dont work best with large dimensionality\n",
    "# hence instead of Onehot encoding - PESPONSE coding used with 9 features\n",
    "\n",
    "alpha = [5, 11, 15, 21, 31, 41, 51, 99]\n",
    "cv_log_error_array = []\n",
    "for i in alpha:\n",
    "    print(\"for alpha =\", i)\n",
    "    clf = KNeighborsClassifier(n_neighbors=i)\n",
    "    clf.fit(train_x_responseCoding, train_y)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(train_x_responseCoding, train_y)\n",
    "    sig_clf_probs = sig_clf.predict_proba(cv_x_responseCoding)\n",
    "    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n",
    "    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n",
    "    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n",
    "\n",
    "\n",
    "\n",
    "best_alpha = np.argmin(cv_log_error_array)\n",
    "clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\n",
    "clf.fit(train_x_responseCoding, train_y)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(train_x_responseCoding, train_y)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(train_x_responseCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(cv_x_responseCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(test_x_responseCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (With Class Balancing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model complexity increases downword ->\n",
    "\n",
    "# Naive Bays (can deal with large dimesionality) \n",
    "# KNN (can't deal with large dimesionality)\n",
    "# Logisctic Regg(can deal with large dimesionality) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss : 1.3841504995321026 for alpha = 1e-06\n",
      "Log Loss : 1.3739018751137557 for alpha = 1e-05\n",
      "Log Loss : 1.3084758548947837 for alpha = 0.0001\n",
      "Log Loss : 1.085104533444946 for alpha = 0.001\n",
      "Log Loss : 1.1412693442313302 for alpha = 0.01\n",
      "Log Loss : 1.512961011189445 for alpha = 0.1\n",
      "Log Loss : 1.7310198582185214 for alpha = 1\n",
      "Log Loss : 1.7519279274949409 for alpha = 10\n",
      "Log Loss : 1.7540776024928375 for alpha = 100\n"
     ]
    }
   ],
   "source": [
    "alpha = [10 ** x for x in range(-6, 3)]\n",
    "cv_log_error_array = []\n",
    "\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(class_weight='balanced', alpha=i, penalty='l2', loss='log', random_state=42)\n",
    "    clf.fit(train_x_onehotCoding, train_y)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(train_x_onehotCoding, train_y)\n",
    "    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n",
    "    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n",
    "    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs),\"for alpha =\", i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For values of best alpha =  0.001 The train log loss is: 0.6430440608605126\n",
      "For values of best alpha =  0.001 The cross validation log loss is: 1.085104533444946\n",
      "For values of best alpha =  0.001 The test log loss is: 1.0647439432231667\n"
     ]
    }
   ],
   "source": [
    "# Trainning model with best alpha = 0.001 \n",
    "\n",
    "best_alpha = np.argmin(cv_log_error_array)\n",
    "clf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
    "clf.fit(train_x_onehotCoding, train_y)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(train_x_onehotCoding, train_y)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(train_x_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(cv_x_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(test_x_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (Without Class Balancing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss : 1.348756545123912 for alpha = 1e-06\n",
      "Log Loss : 1.3383874533988211 for alpha = 1e-05\n",
      "Log Loss : 1.2944675298354409 for alpha = 0.0001\n",
      "Log Loss : 1.1189397576709106 for alpha = 0.001\n",
      "Log Loss : 1.2022337139591928 for alpha = 0.01\n",
      "Log Loss : 1.3830852093239379 for alpha = 0.1\n",
      "Log Loss : 1.6150790873769503 for alpha = 1\n",
      "Log Loss : 1.656181194272004 for alpha = 10\n",
      "Log Loss : 1.6606932992385937 for alpha = 100\n"
     ]
    }
   ],
   "source": [
    "alpha = [10 ** x for x in range(-6, 3)]\n",
    "cv_log_error_array = []\n",
    "\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n",
    "    clf.fit(train_x_onehotCoding, train_y)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(train_x_onehotCoding, train_y)\n",
    "    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n",
    "    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n",
    "    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs),\"for alpha =\", i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For values of best alpha =  0.001 The train log loss is: 0.6321985273917882\n",
      "For values of best alpha =  0.001 The cross validation log loss is: 1.1189397576709106\n",
      "For values of best alpha =  0.001 The test log loss is: 1.0811683048362903\n"
     ]
    }
   ],
   "source": [
    "# Trainning model with best alpha = 0.001 \n",
    "\n",
    "best_alpha = np.argmin(cv_log_error_array)\n",
    "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\n",
    "clf.fit(train_x_onehotCoding, train_y)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(train_x_onehotCoding, train_y)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(train_x_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(cv_x_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(test_x_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM (with ClassBalanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RBK and Kernal SVM not used as we dont know the right kernal\n",
    "* in case we used text or RBF kernal the trained model is not interpretable\n",
    "* In linear SVM its easy to get feature imporatance, they are just like the logistic regg.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss : 1.3575829531084793 for alpha = 1e-06\n",
      "Log Loss : 1.3667828818906311 for alpha = 1e-05\n",
      "Log Loss : 1.316344250040108 for alpha = 0.0001\n",
      "Log Loss : 1.2242403052757578 for alpha = 0.001\n",
      "Log Loss : 1.172015362458566 for alpha = 0.01\n",
      "Log Loss : 1.3986220765454684 for alpha = 0.1\n",
      "Log Loss : 1.7449821872000097 for alpha = 1\n",
      "Log Loss : 1.754425392450009 for alpha = 10\n",
      "Log Loss : 1.7544253965147187 for alpha = 100\n"
     ]
    }
   ],
   "source": [
    "alpha = [10 ** x for x in range(-6, 3)]\n",
    "cv_log_error_array = []\n",
    "\n",
    "for i in alpha:\n",
    "    clf = SGDClassifier(class_weight='balanced',alpha=i, penalty='l2', loss='hinge', random_state=42)\n",
    "    clf.fit(train_x_onehotCoding, train_y)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(train_x_onehotCoding, train_y)\n",
    "    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n",
    "    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n",
    "    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs),\"for alpha =\", i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For values of best alpha =  0.01 The train log loss is: 0.7763267349063716\n",
      "For values of best alpha =  0.01 The cross validation log loss is: 1.172015362458566\n",
      "For values of best alpha =  0.01 The test log loss is: 1.1348984683888912\n"
     ]
    }
   ],
   "source": [
    "# Trainning model with best alpha = 0.001 \n",
    "\n",
    "best_alpha = np.argmin(cv_log_error_array)\n",
    "clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42, class_weight='balanced')\n",
    "clf.fit(train_x_onehotCoding, train_y)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(train_x_onehotCoding, train_y)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(train_x_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(cv_x_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(test_x_onehotCoding)\n",
    "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of CalibratedClassifierCV(base_estimator=SGDClassifier(alpha=0.01, average=False, class_weight='balanced',\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=None,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=42, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False),\n",
       "            cv='warn', method='sigmoid')>"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_clf.get_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier (With Onehot Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logg loss =  1.2473383364263193  at Max_depth =  5  and No of base_learners =  100\n",
      "Logg loss =  1.1938531924106026  at Max_depth =  10  and No of base_learners =  100\n",
      "Logg loss =  1.245205848434237  at Max_depth =  5  and No of base_learners =  200\n",
      "Logg loss =  1.1903654274216724  at Max_depth =  10  and No of base_learners =  200\n",
      "Logg loss =  1.2442697963584366  at Max_depth =  5  and No of base_learners =  500\n",
      "Logg loss =  1.1814622975251694  at Max_depth =  10  and No of base_learners =  500\n",
      "Logg loss =  1.2379356765803096  at Max_depth =  5  and No of base_learners =  1000\n",
      "Logg loss =  1.1761730356562223  at Max_depth =  10  and No of base_learners =  1000\n",
      "Logg loss =  1.2397806154277424  at Max_depth =  5  and No of base_learners =  2000\n",
      "Logg loss =  1.1733295762886249  at Max_depth =  10  and No of base_learners =  2000\n"
     ]
    }
   ],
   "source": [
    "base_learners = [100,200,500,1000,2000]\n",
    "tree_depth = [5,10]\n",
    "cv_log_error_array = []\n",
    "for base in base_learners:\n",
    "    for depth in tree_depth:\n",
    "            clf = RandomForestClassifier(n_estimators=base, criterion='gini', max_depth=depth, n_jobs=-1, random_state=42)\n",
    "            clf.fit(train_x_onehotCoding , y_train)\n",
    "            sig_clf = CalibratedClassifierCV(clf, method='sigmoid')\n",
    "            sig_clf.fit(train_x_onehotCoding, y_train)\n",
    "            sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n",
    "            cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For values of best estimator =  2000 The train log loss is: 0.7236570293240685\n",
      "For values of best estimator =  2000 The cross validation log loss is: 1.1733295762886249\n",
      "For values of best estimator =  2000 The test log loss is: 1.1885092967243407\n"
     ]
    }
   ],
   "source": [
    "# trainning model with best hyperparameter\n",
    "# base_learners = 2000   , tree_depth = 10\n",
    "base_learners = 2000\n",
    "tree_depth = 10\n",
    "clf = RandomForestClassifier(n_estimators=2000 , criterion='gini', max_depth=10 , random_state=42, n_jobs=-1)\n",
    "clf.fit(train_x_onehotCoding, train_y)\n",
    "sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "sig_clf.fit(train_x_onehotCoding, train_y)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(train_x_onehotCoding)\n",
    "print('For values of best estimator = ', base_learners, \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(cv_x_onehotCoding)\n",
    "print('For values of best estimator = ', base_learners, \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n",
    "predict_y = sig_clf.predict_proba(test_x_onehotCoding)\n",
    "print('For values of best estimator = ', base_learners, \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression :  Log Loss: 1.10\n",
      "Support vector machines : Log Loss: 1.75\n",
      "Naive Bayes : Log Loss: 1.30\n",
      "--------------------------------------------------\n",
      "Stacking Classifer : for the value of alpha: 0.000100 Log Loss: 2.179\n",
      "Stacking Classifer : for the value of alpha: 0.001000 Log Loss: 2.046\n",
      "Stacking Classifer : for the value of alpha: 0.010000 Log Loss: 1.557\n",
      "Stacking Classifer : for the value of alpha: 0.100000 Log Loss: 1.165\n",
      "Stacking Classifer : for the value of alpha: 1.000000 Log Loss: 1.236\n",
      "Stacking Classifer : for the value of alpha: 10.000000 Log Loss: 1.480\n"
     ]
    }
   ],
   "source": [
    "clf1 = SGDClassifier(alpha=0.001, penalty='l2', loss='log', class_weight='balanced', random_state=0)\n",
    "clf1.fit(train_x_onehotCoding, train_y)\n",
    "sig_clf1 = CalibratedClassifierCV(clf1, method=\"sigmoid\")\n",
    "sig_clf1.fit(train_x_onehotCoding, train_y)\n",
    "print(\"Logistic Regression :  Log Loss: %0.2f\" % (log_loss(cv_y, sig_clf1.predict_proba(cv_x_onehotCoding))))\n",
    "\n",
    "\n",
    "clf2 = SGDClassifier(alpha=1, penalty='l2', loss='hinge', class_weight='balanced', random_state=0)\n",
    "clf2.fit(train_x_onehotCoding, train_y)\n",
    "sig_clf2 = CalibratedClassifierCV(clf2, method=\"sigmoid\")\n",
    "sig_clf2.fit(train_x_onehotCoding, train_y)\n",
    "print(\"Support vector machines : Log Loss: %0.2f\" % (log_loss(cv_y, sig_clf2.predict_proba(cv_x_onehotCoding))))\n",
    "\n",
    "\n",
    "clf3 = MultinomialNB(alpha=0.001)\n",
    "clf3.fit(train_x_onehotCoding, train_y)\n",
    "sig_clf3 = CalibratedClassifierCV(clf3, method=\"sigmoid\")\n",
    "sig_clf3.fit(train_x_onehotCoding, train_y)\n",
    "print(\"Naive Bayes : Log Loss: %0.2f\" % (log_loss(cv_y, sig_clf3.predict_proba(cv_x_onehotCoding))))\n",
    "\n",
    "print(\"-\"*50)\n",
    "\n",
    "alpha = [0.0001,0.001,0.01,0.1,1,10] \n",
    "best_alpha = 999\n",
    "for i in alpha:\n",
    "    lr = LogisticRegression(C=i)\n",
    "    sclf = StackingClassifier(classifiers=[sig_clf1, sig_clf2, sig_clf3], meta_classifier=lr, use_probas=True)\n",
    "    sclf.fit(train_x_onehotCoding, train_y)\n",
    "    print(\"Stacking Classifer : for the value of alpha: %f Log Loss: %0.3f\" % (i, log_loss(cv_y, sclf.predict_proba(cv_x_onehotCoding))))\n",
    "    log_error =log_loss(cv_y, sclf.predict_proba(cv_x_onehotCoding))\n",
    "    if best_alpha > log_error:\n",
    "        best_alpha = log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss (train) on the stacking classifier : 0.6925169651269207\n",
      "Log loss (CV) on the stacking classifier : 1.1647871909974135\n",
      "Log loss (test) on the stacking classifier : 1.122875556056284\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=0.1)\n",
    "sclf = StackingClassifier(classifiers=[sig_clf1, sig_clf2, sig_clf3], meta_classifier=lr, use_probas=True)\n",
    "sclf.fit(train_x_onehotCoding, train_y)\n",
    "\n",
    "log_error = log_loss(train_y, sclf.predict_proba(train_x_onehotCoding))\n",
    "print(\"Log loss (train) on the stacking classifier :\",log_error)\n",
    "\n",
    "log_error = log_loss(cv_y, sclf.predict_proba(cv_x_onehotCoding))\n",
    "print(\"Log loss (CV) on the stacking classifier :\",log_error)\n",
    "\n",
    "log_error = log_loss(test_y, sclf.predict_proba(test_x_onehotCoding))\n",
    "print(\"Log loss (test) on the stacking classifier :\",log_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
